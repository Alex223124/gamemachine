
gamemachine {

  environment = test
  use_regions = true
  orm = true
  mono_enabled = false
  admin_enabled = false

  client {

    # Sets the protocol and host/port used by the client if set to TCP or UDP.  ANY lets the client use it's own values
    protocol = ANY

    idle_timeout = 20
  }

   handlers {

    # Team handler
    team = "GameMachine::DefaultHandlers::TeamHandler"

    # Authentication handlers.  

    #PublicAuthenticator allows any user/password combination to work
    auth = "com.game_machine.authentication.PublicAuthenticator"

    # The default.  Uses salted password hash (bcrypt)
    #auth = "com.game_machine.authentication.DefaultAuthenticator"

    # Database authentication. Triggers player data to be saved in the database
    #auth = "com.game_machine.authentication.DbAuthenticator"


  }
  
  # These settings control the number of actors for each of the routers, and effect throughput.
  # Generally, game_handler, request_handler, and udp should be the same, with objectdb set 2-3 times higher.
  routers {
    # Size of the router for incoming message from all clients
    game_handler = 5

    # Also in the pipeline for all incoming requests, should be set the same as
    # game_handler_routers
    request_handler = 5

    # router size for the actor that handles all incoming udp/tcp messages
    incoming = 5

    objectdb =  50
  }
  
  datastore {
    # What data store the object database will use (one of memory, mapdb, or couchbase)
    store = gamecloud

    serialization = bytes

    # Write behind cache
    # Both cache_write_interval and cache_writes_per_second must be uncommented for write caching to be enabled.

    # minimum time in milliseconds between writes to the backing store of the same key.  When a message
    # exceeds the limit it is either enqueued, or if already enqueued, it's value is updated.
    cache_write_interval = 1000

    # Total writes per second to the backing store that an actor will allow, across all keys.  Note this is on a per actor
    # basis.  The total writes per second is routers.objectdb * cache_writes_per_second.

    # Note that actual writes per second will be somewhat lower then the number you enter here after accounting for
    # overall latency.  On average actual writes are 20% below the target.  
    cache_writes_per_second = 30
  }
  

  gamecloud {
    #host = "localhost:9000"
    #user = test
    #api_key = 294122207690d6e93867c56fae265c24
    #api_key = 20934a81ba104ad1936b8a3e9e7fdaef

    user = chris
     host = "cloud-dev.gamemachine.io:80"
    api_key = 1a77d75e32767f89b0710311be1fd8a5
  }

  grids {
    default = "4000, 50, 1"
    aoe =  "4000, 5, 1"
    local_chat =  "4000, 10, 10"
  }

  couchbase {
    bucket = gamemachine
    password = pass
    servers = ["http://192.168.1.8:8091/pools"]
  }

  jdbc {
    
    // Postgres
    hostname = 127.0.0.1
    port = 5432
    database = gamemachine
    url = "jdbc:postgresql://127.0.0.1:5432/gamemachine"
    ds = "org.postgresql.ds.PGSimpleDataSource"
    driver = "org.postgresql.Driver"
    username = gamemachine
    password = gamemachine

    // Mysql
    #hostname = 127.0.0.1
    #port = 3306
    #database = gamemachine
    #url = "mysql://127.0.0.1:3306/game_machine"
    #ds = "com.mysql.jdbc.jdbc2.optional.MysqlDataSource"
    #driver = "com.mysql.jdbc.Driver"
    #username = gamemachine
    #password = gamemachine

  }

  http {
    enabled = true
    host = 0.0.0.0
    port = 3000
  }

  udp {
    enabled = true
    host = 0.0.0.0
    port = 24130
  }

  tcp {
    enabled = true
    host = 0.0.0.0
    port = 8910
  }

  akka {
    host = 127.0.0.1
    port = 2551
  }

  # Seeds are akka nodes that should already be in the cluster.  We use them as first points of contact to join against
  # If there are no seeds, we start the cluster by joining to ourself
  #seeds = ["127.0.0.1:9991","127.0.0.1:9992"]
  seeds = []

  admin {
    user = admin
    pass = pass
  }
  
  regions = [
    ["zone1","GameMachine::DefaultHandlers::ZoneManager"],
    ["zone2", "GameMachine::DefaultHandlers::ZoneManager"]
  ]
    
}
 
 